<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE knimeNode PUBLIC "-//UNIKN//DTD KNIME Node 2.0//EN" "http://www.knime.org/Node.dtd">
<knimeNode icon="treeensemble_learner_regression.png" type="Learner">
	<name>Simple Regression Tree Learner</name>

	<shortDescription>
		Learns a single regression tree.
	</shortDescription>

	<fullDescription>
		<intro>
			<p>
				Learns a single regression tree. The procedure follows the algorithm
				described by
				"Classification and Regression Trees" (Breiman et al, 1984), whereby the current
				implementation applies a
				couple of simplifications, e.g. no pruning, not necessarily binary trees,
				etc.
			</p>
			<p>
        		In a regression tree the predicted value for a leaf node is the mean target value of the records within the leaf.
        		Hence the predictions are best (with respect to the training data) if the variance of target values within a leaf is minimal.
        		This is achieved by splits that minimize the sum of squared errors in their respective children.
      		</p>
			<p>
				The currently used missing value handling also differs from the one
				used by Breiman et al, 1984.
				In each split the algorithm tries to find the best direction for
				missing values by sending them in each direction and selecting
				the one that yields the best result (i.e. largest gain). The procedure
				is adapted from the well known XGBoost algorithm and is described
				<a href="https://github.com/dmlc/xgboost/issues/21">here</a>
				.
			</p>
		</intro>
		<tab name="Attribute Selection">
			<option name="Target Column">
				Select the column containing the value to be learned. Rows with missing
				values in this column will be ignored
				during the learning process.
			</option>
			<option name="Attribute Selection">
				<p>Select the attributes on which the model should be learned. You can choose from two modes.
				</p>
				<p>
					<i>Fingerprint attribute</i>
					Uses a fingerprint/vector (bit, byte and double are possible) column to learn the model by treating
					each entry of the vector as separate attribute (e.g. a bit vector of length 1024 is expanded into 1024 binary attributes).
					The node requires all vectors to be of the same length.
				</p>
				<p>
					<i>Column attributes</i>
					Uses ordinary columns in your table (e.g. String, Double, Integer, etc.) as attributes to learn the model on.
					The dialog allows to select the columns manually (by moving them to the right panel) or via a wildcard/regex selection
					(all columns whose names match the wildcard/regex are used for learning).
					In case of manual selection, the behavior for new columns (i.e. that are not available at the time you configure the node)
					can be specified as either <i>Enforce exclusion</i> (new columns are excluded and therefore not used for learning) or 
					<i>Enforce inclusion</i> (new columns are included and therefore used for learning).
				</p>
			</option>
			<option name="Ignore columns without domain information">
				If selected, nominal columns with no domain information are ignored
				(as they likely have too many possible
				values anyway).
			</option>
			<option name="Enable Hightlighting (#patterns to store)">
				If selected, the node stores the selected number of rows and allows
				highlighting them in the node view.
			</option>
			<option name="Use binary splits for nominal attributes">
				If selected, the node determines set based binary splits for nominal values.
				Otherwise each value will result in a child node.
			</option>
			<option name="Missing value handling">Here the preferred missing value handling can be specified there are the following options:
				<ul>
					<li>XGBoost - If this is selected (it is also the default), the learner will calculate which direction is best suited for
					missing values, by sending the missing values in each direction of a split. The direction that yields the best result (i.e. largest gain)
					is then used as default direction for missing values. This method works with both, binary and multiway splits.</li>
					<li>Surrogate - This approach calculates for each split alternative splits that best approximate the best split.
					The method was first described in the book "Classification and Regression Trees" by Breiman et al. (1984).
					NOTE: This method can only be used with binary nominal splits.</li>
				</ul>
			</option>
			<option name="Limit number of levels (tree depth)">
				Number of tree levels to be learned. For instance, a value of 1 would only
				split the (single) root node
				(decision stump).
			</option>
			<option name="Minimum split node size">Minimum number of records in a decision tree node
				so that another split
				is attempted. Note, this option does not make any implications on the
				minimum number of records in a
				terminal node. If enabled, this number needs to be at least twice as large
				as the minimum child node size
				(as otherwise for binary splits one of the two children would have less
				records than specified).
			</option>
			<option name="Minimum node size">Minimum number of records in child nodes. It can
				be at most half of
				the minimum split node size (see above). Note, this parameter is
				currently ignored for nominal splits.
			</option>
		</tab>
	</fullDescription>
	<ports>
		<inPort index="0" name="Input Data">The data to learn from. It must contain
			at least one numeric target column and
			either a fingerprint (bit/byte/double-vector) column or another numeric or
			nominal column.
		</inPort>
		<outPort index="0" name="Regression Tree Model">The trained model.</outPort>
	</ports>
	<views>
		<view name="Regression Tree View" index="0">Regression Tree View</view>
	</views>
</knimeNode>
